【开场白 - 约500字】

大家好，欢迎收听本期技术深度播客。我是你的主播，今天我们要聊的话题是 Inference Storage —— 推理存储优化。这是大模型时代一个极其重要，但却经常被忽视的技术领域。

如果说 GPU 算力是人工智能的心脏，那 Inference Storage 就是支撑整个系统的血管网络。没有高效的存储系统，再强大的算力也无法发挥。2026年初，这个领域正在经历一场技术革命。从 KV Cache 的智能卸载，到 Prefill 和 Decode 的分离部署，再到 MoE 模型的宽专家并行，每一项技术都在重新定义大模型推理的性能边界。

就在上个月，Meta 公开了他们基于 vLLM 的生产级推理架构，实现了数十倍的性能提升。英伟达发布了全新的 Dynamo 框架，号称能在 GB200 上实现 DeepSeek-R1 推理吞吐提升 30 倍。vLLM 团队则完成了对 Blackwell 架构的深度优化，让 MoE 模型的推理效率达到了前所未有的高度。

这些技术不是遥远的实验室概念，它们正在走进生产环境，影响着每一个使用大模型的企业和开发者。无论你是 AI 基础设施的工程师，还是对技术趋势感兴趣的从业者，今天的节目都会带给你深度的技术洞察。

准备好了吗？让我们开始今天的技术之旅。

【第一部分：KV Cache — 推理存储的核心挑战 - 约1100字】

要理解 Inference Storage，我们必须先理解 KV Cache。

在 Transformer 架构中，模型生成文本是一个自回归的过程，简单来说就是一个字一个字地往外蹦。每生成一个新 token，模型都要回头看之前所有的内容，计算注意力关系。如果不做任何优化，每次都要重新计算前面所有 token 的 Key 和 Value 矩阵，那计算量就是平方级增长，根本没法用。

于是工程师们想出了一个绝妙的办法：把 Key 和 Value 矩阵缓存下来。这就是 KV Cache —— 键值缓存。第一次计算完成后，后续的生成只需要读取缓存，而不需要重复计算。这个简单的优化让 Transformer 推理从不可能变成了可能。

但 KV Cache 带来了一个新的问题：存储爆炸。

让我们算一笔账。以 Llama-2-70B 为例，模型维度是 8192，有 80 层注意力。对于一个序列长度 4096 的请求，KV Cache 需要多少显存？答案是大约 10GB。如果 batch size 是 16，那就是 160GB。而一张 A100 只有 80GB 显存，模型权重本身还要占掉 140GB。这意味着，即使是最顶级的显卡，也只能同时处理几个中等长度的请求。

更糟糕的是，KV Cache 的碎片化问题。不同的用户请求长度差异巨大，有的只有十几个字，有的可能几万个 token。传统做法是预先分配最大长度的连续内存块，这导致显存利用率只有百分之四十到五十。一半以上的显存就这样被浪费了。

Inference Storage 面临的挑战可以总结为三个维度。

第一是容量挑战。长上下文场景下，单个请求的 KV Cache 可能达到数十 GB。如何在有限的 GPU 显存中容纳更多的并发请求，是系统设计的首要难题。

第二是带宽挑战。Decode 阶段是内存密集型的，每生成一个 token 都需要从显存中读取全部的 KV Cache。如果带宽不足，GPU 计算单元就会处于空闲状态，造成严重的资源浪费。

第三是延迟挑战。在多轮对话中，用户希望模型能够快速响应。但如果 KV Cache 管理不当，每次请求都要重新计算，那用户体验就会大打折扣。

面对这些挑战，工程师们发展出了多种优化技术。接下来我们要聊的 PagedAttention，就是其中最核心的突破。

【第二部分：PagedAttention 与内存管理革命 - 约1100字】

面对 KV Cache 的碎片化问题，加州大学伯克利分校的 vLLM 团队提出了一个天才般的解决方案：PagedAttention。

这个技术的核心思想非常简单：借鉴操作系统的虚拟内存分页机制。

在操作系统中，物理内存被划分成固定大小的页框，每个进程看到的是虚拟地址空间。虚拟地址到物理地址的映射由页表维护，物理页面不需要连续，可以分散在内存的各个角落。

vLLM 把这个思想搬到了 GPU 显存管理中。他们把 KV Cache 分割成固定大小的 block，通常是每 16 个 token 一个 block。系统维护一个虚拟到物理的映射表，block 在物理上不需要连续存储。

这个设计的优势是巨大的。

第一，消除了内部碎片。传统的预分配方式会为每个请求预留最大可能的长度，而 PagedAttention 是按需分配，用多少分配多少，显存利用率从百分之四五十提升到了百分之九十以上。

第二，实现了内存共享。当多个请求有公共前缀时，比如系统提示词或者多轮对话的历史记录，它们可以共享相同的 KV Cache block。vLLM 使用写时复制技术，只有当某个请求需要修改共享内容时，才会创建私有副本。这在批量处理相似请求时能带来数倍的性能提升。

第三，灵活的内存管理。block 级别的分配让系统能够更灵活地应对动态负载。当显存不足时，系统可以把不常用的 block 卸载到 CPU 内存，或者丢弃低优先级的请求，而不影响其他请求的执行。

PagedAttention 的实现需要深入到 CUDA kernel 层面。vLLM 团队重写了 attention 计算的核心逻辑，使其能够处理非连续的 KV Cache 布局。他们还优化了内存访问模式，确保即使数据分散存储，GPU 也能高效地读取。

从实际效果来看，PagedAttention 是一个游戏规则改变者。在相同的硬件条件下，使用 vLLM 的系统可以处理的并发请求数量是传统方案的两到三倍。这意味着企业可以用一半的 GPU 成本支撑相同的业务量，或者用相同的成本服务更多的用户。

更重要的是，PagedAttention 为后续的优化奠定了基础。KV Cache 的卸载、前缀共享、动态批处理，这些高级特性都建立在分页存储的基础之上。

【第三部分：分层存储与 KV Cache Offloading - 约1100字】

PagedAttention 解决了显存内部的管理效率问题，但它无法突破显存容量的物理限制。当模型规模越来越大，上下文长度越来越长，GPU 显存终究还是不够用。

这时候，分层存储架构就派上用场了。

分层存储的核心思想是：不是所有的数据都需要一直待在 GPU 里。根据访问频率和延迟要求，把 KV Cache 放在不同层级的存储介质中，形成一个金字塔结构。

最顶层是 GPU 的 HBM，也就是高带宽内存。这是速度最快的地方，纳秒级的访问延迟，几 TB 每秒的带宽。这里存放的是 Hot Cache —— 正在处理中的请求的 KV Cache。

第二层是 CPU 的 DRAM。虽然比 GPU 显存慢一些，但容量可以大得多。一台服务器可以轻松配置几百 GB 甚至几 TB 的内存。这里存放的是 Warm Cache —— 近期可能用到的 KV Cache，比如被抢占的请求或者等待中的多轮对话历史。

第三层是本地的 NVMe SSD。延迟在微秒级别，容量可以达到几十 TB。这里可以存放 Cold Storage —— 不常访问但仍然有价值的历史数据。

最底层是远程存储，比如对象存储或者分布式文件系统。延迟在毫秒级别，但容量几乎无限。这里存放的是 Archive —— 需要长期保留的会话记录。

2026 年 1 月，vLLM 发布了 KV Cache Offloading Connector，正式支持将 KV Cache 异步卸载到 CPU 内存。这个功能的实现非常精巧。

首先是异步 API 设计。vLLM 0.9.0 引入了异步 Connector 接口，数据传输在后台进行，不会阻塞推理引擎的计算流水线。这意味着 GPU 可以专注于生成 token，而 CPU 负责数据的搬运。

其次是可插拔的后端架构。系统支持任何介质的 KV 数据传输，开发者只需要实现简单的接口就可以把数据送到想要的地方。vLLM 内置了原生的 CPU DRAM 后端，开箱即用。

实际的收益是显著的。一台配备 8 张 H200 的服务器，GPU 显存总共只有约 1.1 TB，但 CPU 内存可以轻松达到 2 TB 甚至更多。启用 Offloading 后，系统可以缓存更多的历史对话，减少重复计算，同时支持更长的上下文窗口。

对于生产环境来说，Offloading 还有一个重要的作用：降低请求抢占的开销。当显存不足时，传统做法是把低优先级的请求踢出去，丢弃它们的 KV Cache。当这些请求恢复时，需要重新计算，造成巨大的延迟。有了 Offloading，被抢占的请求可以把 KV Cache 卸载到 CPU，恢复时直接加载回来，无需重新计算。

NVIDIA 的 Dynamo 框架更进一步，实现了多级内存卸载。除了 GPU 和 CPU，还支持将 KV Cache 卸载到外部存储。这在处理超大规模上下文时特别有用，比如处理整本书或者大量的代码库。

【第四部分：Disaggregated Serving — 分离式推理架构 - 约1000字】

如果说分层存储解决了容量问题，那 Disaggregated Serving 解决的是资源利用效率问题。

Disaggregated Serving，中文通常翻译为分离式推理或者解耦式部署。它的核心洞察是：大模型推理的两个阶段，Prefill 和 Decode，有着截然不同的资源需求。

Prefill 阶段是输入处理阶段。模型需要读取用户的整个输入提示，计算所有 token 的注意力关系，生成第一个输出 token。这个阶段是计算密集型的，需要大量的 Tensor Core 进行矩阵运算。输入越长，需要的计算量越大。

Decode 阶段是输出生成阶段。模型一个 token 一个 token 地生成文本，每生成一个 token 都要读取全部的 KV Cache。这个阶段是内存密集型的，瓶颈在于显存带宽而不是计算能力。

关键问题是：这两个阶段的资源需求是冲突的。Prefill 需要大量的计算核心，Decode 需要高显存带宽。如果在同一张 GPU 上同时运行两者，就会出现资源争抢。当 Prefill 在全力计算时，Decode 可能因为带宽不足而停滞；当 Decode 在密集读取内存时，Prefill 的计算单元可能在空转。

分离式架构的解决方案是：把 Prefill 和 Decode 放在不同的 GPU 上。

具体的架构是这样的。请求首先到达一个智能路由器，路由器把请求发送到 Prefill 集群。Prefill 集群的 GPU 专注于计算，快速处理输入并生成第一个 token。同时，Prefill 会把计算好的 KV Cache 通过网络传输给 Decode 集群。

Decode 集群的 GPU 专注于生成后续 token。它们接收 KV Cache 后，继续完成剩余的生成任务。由于 Decode 是内存密集型的，这些 GPU 可以配置更高的显存带宽，或者运行更大的 batch size。

Meta 基于 vLLM 实现了这套架构，并在生产环境中验证了效果。根据 PyTorch 官方博客披露的数据，分离式架构相比传统的混合部署，在 TTFT 和 TTIT 两个关键指标上都有显著提升。

TTFT，Time To First Token，指的是从请求发出到收到第一个 token 的时间。这个指标影响用户的感知延迟，越短越好。分离式架构通过专门的 Prefill 集群，把这个时间降低了百分之四十以上。

TTIT，Time To Iterative Token，指的是迭代生成阶段每个 token 的平均时间。这个指标影响整体的吞吐量。分离式架构让 Decode 集群可以更高效地利用显存带宽，吞吐量提升了两到五倍。

NVIDIA 的 Dynamo 框架进一步增强了分离式架构。它引入了动态调度能力，根据实时负载情况自动调整 Prefill 和 Decode 集群的规模。当流量高峰期到来，系统自动扩容 Decode 集群；当输入长度普遍较长，系统增加 Prefill 集群的资源。这种弹性能力对于应对生产环境的流量波动至关重要。

【结尾 - 约400字】

好的，让我们回顾一下今天聊到的内容。

Inference Storage 是大模型推理的基础设施，KV Cache 是其核心数据结构。我们讨论了三大技术方向：

第一，PagedAttention 通过分页机制解决了显存碎片化问题，将利用率从百分之四五十提升到了百分之九十以上。

第二，分层存储和 KV Cache Offloading 突破了 GPU 显存的容量限制，通过把数据卸载到 CPU 内存甚至外部存储，支持了更大的上下文窗口和更多的并发请求。

第三，Disaggregated Serving 分离式架构通过把 Prefill 和 Decode 阶段解耦，消除了资源冲突，显著提升了推理的延迟和吞吐性能。

这些技术不是孤立存在的，它们可以组合使用。一个理想的推理系统可能同时采用 PagedAttention 进行内存管理，使用 Offloading 扩展容量，部署分离式架构优化资源利用，再配上智能的路由和调度策略。

展望未来，Inference Storage 还有很大的发展空间。CXL 内存扩展技术有望让 CPU 和 GPU 共享统一的内存空间，消除数据拷贝的开销。存算一体架构可能彻底改变存储和计算的关系。语义感知的缓存策略可以让系统更智能地决定哪些数据值得保留。

对于正在构建 AI 基础设施的团队，我的建议是：密切关注 vLLM 的发展，它正在成为行业的事实标准。如果预算允许，考虑升级到 Blackwell 架构，配合 NVFP4 量化可以获得巨大的性能提升。对于大规模生产环境，分离式架构值得投入精力去实现。

好了，今天的节目就到这里。如果你觉得这期内容有价值，欢迎分享给你的朋友和同事。我们下期再见。
